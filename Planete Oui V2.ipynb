{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from elm2 import ELMClassifier, ELMRegressor, GenELMClassifier, GenELMRegressor\n",
    "from random_layer import RandomLayer, MLPRandomLayer, RBFRandomLayer, GRBFRandomLayer\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.utils import shuffle\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import datetime as dt\n",
    "import math as mt\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_nilm(dataframe_y_true, dataframe_y_pred):\n",
    "    score = 0.0\n",
    "    test = dataframe_y_true[~dataframe_y_true['washing_machine'].isna()]['washing_machine']\n",
    "    pred = dataframe_y_pred[~dataframe_y_true['washing_machine'].isna()]['washing_machine']\n",
    "    score += mt.sqrt(sum((pred - test)**2)/len(test))*5.55\n",
    "    test = dataframe_y_true[~dataframe_y_true['fridge_freezer'].isna()]['fridge_freezer']\n",
    "    pred = dataframe_y_pred[~dataframe_y_true['fridge_freezer'].isna()]['fridge_freezer']\n",
    "    score += mt.sqrt(sum((pred - test)**2)/len(test))*49.79\n",
    "    test = dataframe_y_true[~dataframe_y_true['TV'].isna()]['TV']\n",
    "    pred = dataframe_y_pred[~dataframe_y_true['TV'].isna()]['TV']\n",
    "    score += mt.sqrt(sum((pred - test)**2)/len(test))*14.57\n",
    "    test = dataframe_y_true[~dataframe_y_true['kettle'].isna()]['kettle']\n",
    "    pred = dataframe_y_pred[~dataframe_y_true['kettle'].isna()]['kettle']\n",
    "    score += mt.sqrt(sum((pred - test)**2)/len(test))*4.95\n",
    "    score /= 74.86\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"X_train.csv\")\n",
    "X_train = X_train[X_train.columns[1:]]\n",
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.read_csv(\"y_train.csv\")\n",
    "y_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(\"X_test.csv\")\n",
    "X_test = X_test[X_test.columns[1:]]\n",
    "X_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.sort_values(by=\"time_step\")\n",
    "y_train = y_train.sort_values(by=\"time_step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Missing value imputaton;\n",
    "2. Derive time features (date, hour, minute, dayofweek, isweekend, hour_sin, hour_cos, minute_sin, minute_cos);\n",
    "3. PCA of weather features;\n",
    "4. Windows with margin (is_kettle, is_washing, kettle_current, washing_current);\n",
    "5. Consumption shifting (3 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only feasible for transforming X (not y)\n",
    "## fe = FeatureExtractor()\n",
    "## X_DF = fe.transform(X_test, leave_null)\n",
    "## Pass leave_null = True for train data,\n",
    "## leave_null = False for test data.\n",
    "\n",
    "class FeatureExtractor(object):\n",
    "    def __init__(self):\n",
    "        self.X_transformed = None\n",
    "\n",
    "    def fit(self, X_df, y_array):\n",
    "        pass\n",
    "\n",
    "    def transform(self, X_df, limit=5, leave_null=True):\n",
    "        \n",
    "        X_transformed = X_df.copy()\n",
    "        \n",
    "        # Missing Value Imputation\n",
    "        sample1 = X_transformed.interpolate(method='linear')\n",
    "        sample1 = sample1.interpolate(method='linear', limit_direction='both')\n",
    "        sample1['consumption'] = X_transformed['consumption']\n",
    "        X_transformed = sample1.copy()\n",
    "        \n",
    "        X_transformed = X_transformed.interpolate(method='nearest', limit_direction='forward', limit=limit)\n",
    "        \n",
    "        # Derive Time Features\n",
    "        X_transformed['date'] = X_transformed['time_step'].str[:10]\n",
    "        X_transformed['hour'] = X_transformed['time_step'].str[11:13].astype('int')\n",
    "        X_transformed['minute'] = X_transformed['time_step'].str[14:16].astype('int')\n",
    "\n",
    "        X_transformed['date'] = pd.to_datetime(X_transformed['date'], infer_datetime_format=True)\n",
    "\n",
    "        X_transformed['dayofweek'] = X_transformed['date'].dt.dayofweek\n",
    "        \n",
    "        isweekend = []\n",
    "        for day in list(X_transformed['dayofweek']):\n",
    "            if day in [5,6]:\n",
    "                isweekend.append(1)\n",
    "            else:\n",
    "                isweekend.append(0)\n",
    "        X_transformed['isweekend'] = isweekend\n",
    "        \n",
    "        X_transformed = pd.concat([X_transformed, pd.get_dummies(X_transformed['dayofweek'], prefix='day'),\n",
    "                           pd.get_dummies(X_transformed['hour'], prefix='hour')], axis=1)\n",
    "        \n",
    "        X_transformed['hour_sin'] = np.sin((X_transformed['hour'] + 1) * 360. / 24. * np.pi / 180. )\n",
    "        X_transformed['hour_cos'] = np.cos((X_transformed['hour'] + 1) * 360. / 24. * np.pi / 180. )\n",
    "\n",
    "        X_transformed['minute_sin'] = np.sin((X_transformed['minute'] + 1) * 360. / 60. * np.pi / 180. )\n",
    "        X_transformed['minute_cos'] = np.cos((X_transformed['minute'] + 1) * 360. / 60. * np.pi / 180. )\n",
    "        \n",
    "        # PCA of Weather Features\n",
    "        PCA_features = ['visibility', 'temperature', 'humidity', 'humidex', 'windchill', 'wind', 'pressure']\n",
    "        PCA_data = X_transformed[PCA_features]\n",
    "\n",
    "        pca = PCA(n_components=3)\n",
    "        pca3_feature = pca.fit_transform(PCA_data)\n",
    "        \n",
    "        X_transformed['WeatherPCA1'] = pca3_feature[:,0]\n",
    "        X_transformed['WeatherPCA2'] = pca3_feature[:,1]\n",
    "        X_transformed['WeatherPCA3'] = pca3_feature[:,2]\n",
    "        \n",
    "        \n",
    "        # Windows with margin\n",
    "        def window_diff(series, start_pos, size, margin):\n",
    "            begin = time()\n",
    "            starting_avg = np.mean(series[start_pos:start_pos+margin])\n",
    "            ending_avg = np.mean(series[start_pos+size-margin:start_pos+size])\n",
    "            end = time()\n",
    "            #print(\"CalcDiff Time: %.5f s\" %(end-begin))\n",
    "            return ending_avg - starting_avg\n",
    "\n",
    "        def window_state(series, start_pos, size, margin, threshold):\n",
    "            diff = window_diff(series, start_pos, size, margin)\n",
    "            if diff > threshold:\n",
    "                return 1\n",
    "            elif diff < -threshold:\n",
    "                return -1\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "        def window_update(series, size, margin, threshold):\n",
    "            begin1 = time()\n",
    "            states = pd.Series(len(series)*[0])\n",
    "            len_series = len(series)\n",
    "            for i in range(len_series-size):\n",
    "                #if i % 10000 == 0:\n",
    "                    #print(i)\n",
    "                begin = time()\n",
    "                states[i] = window_state(series, i, size, margin, threshold)\n",
    "                end = time()\n",
    "                #print(\"Update Time: %.5f s\" %(end-begin))\n",
    "            states[len_series-size:] = window_state(series, len_series-size, size, margin, threshold)\n",
    "            end1 = time()\n",
    "            #print(\"Total Time: %.5f s\" %(end1-begin1))\n",
    "            return states\n",
    "\n",
    "        #######\n",
    "        def OnOff_update(window_series, size):\n",
    "            begin1 = time()\n",
    "            len_series = len(window_series)\n",
    "            states = pd.Series(len_series*[0])\n",
    "            i = 0\n",
    "            while i <= len_series - size:\n",
    "                #if i % 20000 == 0:\n",
    "                    #print(i)\n",
    "                begin = time()\n",
    "                current_state = window_series[i]\n",
    "                next_state = window_series[i+1]\n",
    "                if current_state == 1 and next_state == 0:\n",
    "                    states[i:i+size] = 1\n",
    "                    i = i + size\n",
    "                else:\n",
    "                    i = i + 1\n",
    "                end = time()\n",
    "                #print(\"Update Time: %.5f s\" %(end-begin))\n",
    "            states[i:] = window_series[i]\n",
    "            end1 = time()\n",
    "            #print(\"Total Time: %.5f s\" %(end1-begin1))\n",
    "            return states\n",
    "        \n",
    "        X_transformed['is_kettle_2500'] = window_update(X_transformed['consumption'], 3, 1, 2500)\n",
    "        X_transformed['is_kettle_2700'] = window_update(X_transformed['consumption'], 3, 1, 2700)\n",
    "        X_transformed['is_kettle_2700_5'] = window_update(X_transformed['consumption'], 5, 1, 2700)\n",
    "        X_transformed['is_washing_1800_5'] = window_update(X_transformed['consumption'], 100, 5, 1800)\n",
    "        X_transformed['is_washing_2000_10'] = window_update(X_transformed['consumption'], 100, 10, 2000)\n",
    "        X_transformed['is_washing_2000_5'] = window_update(X_transformed['consumption'], 100, 5, 2000)\n",
    "        \n",
    "        X_transformed['kettle_current1'] = OnOff_update(X_transformed['is_kettle_2500'], 3)\n",
    "        X_transformed['kettle_current2'] = OnOff_update(X_transformed['is_kettle_2700'], 3)\n",
    "        X_transformed['kettle_current3'] = OnOff_update(X_transformed['is_kettle_2700_5'], 5)\n",
    "        X_transformed['washing_current1'] = OnOff_update(X_transformed['is_washing_1800_5'], 100)\n",
    "        X_transformed['washing_current2'] = OnOff_update(X_transformed['is_washing_2000_10'], 100)\n",
    "        X_transformed['washing_current3'] = OnOff_update(X_transformed['is_washing_2000_5'], 100)\n",
    "        \n",
    "        # Consumption Shifting\n",
    "        train['consumption_last1min'] = train['consumption'].shift(1)\n",
    "        train['consumption_last2min'] = train['consumption'].shift(2)\n",
    "        train['consumption_last3min'] = train['consumption'].shift(3)\n",
    "        train['consumption_last4min'] = train['consumption'].shift(4)\n",
    "        train['consumption_last5min'] = train['consumption'].shift(5)\n",
    "    \n",
    "        # Null Value Imputation\n",
    "        if leave_null == False:\n",
    "            X_transformed = X_transformed.fillna(0)\n",
    "        \n",
    "        return X_transformed\n",
    "    \n",
    "    def transform_cols(self, X_df):\n",
    "        return self.X_transformed.columns\n",
    "    \n",
    "    def getDF(self):\n",
    "        return self.X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transform X_train\n",
    "fe = FeatureExtractor()\n",
    "X_train = fe.transform(X_train, leave_null=True)\n",
    "\n",
    "### Transform X_test\n",
    "X_test = fe.transform(X_test, leave_null=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(X_train, y_train,\n",
    "                 how=\"left\",\n",
    "                 left_on=['time_step'],\n",
    "                 right_on=['time_step'],\n",
    "                 sort=False\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "size = 1000\n",
    "start = 2400\n",
    "\n",
    "plot_sample = train.iloc[start:start+size,:]\n",
    "plt.plot(np.arange(0,size,1), plot_sample['consumption'], label='Consumption')\n",
    "plt.plot(np.arange(0,size,1), plot_sample['washing_machine'], label='washing_machine')\n",
    "plt.plot(np.arange(0,size,1), plot_sample['fridge_freezer'], label='fridge_freezer')\n",
    "plt.plot(np.arange(0,size,1), plot_sample['TV'], label='TV', color=\"purple\")\n",
    "plt.plot(np.arange(0,size,1), plot_sample['kettle'], label='kettle', color=\"red\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1,figsize = (20,16))\n",
    "\n",
    "size = 50000\n",
    "plot_sample = train.iloc[:size,:]\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.hist(plot_sample.washing_machine, bins=100, density=True)\n",
    "plt.xlim(0,2500)\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.hist(plot_sample.fridge_freezer, bins=50, density=True)\n",
    "plt.xlim(0,300)\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.hist(plot_sample.TV, bins=50, density=True)\n",
    "plt.xlim(0,150)\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.hist(plot_sample.kettle, bins=10, density=True)\n",
    "plt.xlim(0,3000)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invalid Data Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_notnull = train[train['consumption'].notna()]\n",
    "train_notnull = train_notnull[train_notnull['consumption_last5min'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_notnull.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Set Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vars = ['consumption', 'visibility', 'temperature', 'humidity','humidex',\n",
    "                   'windchill', 'wind', 'pressure']\n",
    "y_vars = ['washing_machine','fridge_freezer','TV','kettle']\n",
    "X_vars_base = ['consumption']\n",
    "X_vars_3mins = ['consumption_last1min', 'consumption_last2min', 'consumption_last3min']\n",
    "X_vars_5mins = ['consumption_last1min', 'consumption_last2min', 'consumption_last3min',\n",
    "               'consumption_last4min', 'consumption_last5min']\n",
    "X_vars_day = ['isweekend', 'day_0', 'day_1',\n",
    "       'day_2', 'day_3', 'day_4', 'day_5', 'day_6']\n",
    "X_vars_hour1 = ['hour_0', 'hour_1',\n",
    "       'hour_2', 'hour_3', 'hour_4', 'hour_5', 'hour_6', 'hour_7', 'hour_8',\n",
    "       'hour_9', 'hour_10', 'hour_11', 'hour_12', 'hour_13', 'hour_14',\n",
    "       'hour_15', 'hour_16', 'hour_17', 'hour_18', 'hour_19', 'hour_20',\n",
    "       'hour_21', 'hour_22', 'hour_23']\n",
    "X_vars_hour2 = ['hour_sin', 'hour_cos']\n",
    "X_vars_minute = ['minute_sin', 'minute_cos']\n",
    "X_vars_weather1 = ['visibility', 'temperature', 'humidity',\n",
    "                   'humidex', 'windchill', 'wind', 'pressure']\n",
    "X_vars_weather2 = ['WeatherPCA1', 'WeatherPCA2', 'WeatherPCA3']\n",
    "X_vars_window1 = ['is_kettle_2500', 'is_washing_1800_5', 'kettle_current1', 'washing_current1']\n",
    "X_vars_window2 = ['is_kettle_2700', 'is_washing_2000_10', 'kettle_current2', 'washing_current2']\n",
    "X_vars_window3 = ['is_kettle_2700_5', 'is_washing_2000_5', 'kettle_current3', 'washing_current3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 - washing_machine; 1 - fridge_freezer; 2 - TV; 3 - kettle;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn.LinearRegression()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vars_set = X_vars_base+X_vars_3mins+X_vars_day+X_vars_hour2+X_vars_minute+X_vars_weather2+X_vars_window3\n",
    "#X_vars_set = X_vars_base+X_vars_3mins+X_vars_day+X_vars_hour2+X_vars_weather2+X_vars_window3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_notnull[['time_step']+X_vars_set]\n",
    "y = train_notnull[y_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1)\n",
    "print('Train size: {train}, Validation size: {test}'.format(train=x_train.shape[0], test=x_valid.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# May lead to very bad results\n",
    "#scaler = StandardScaler()\n",
    "#x_scaled = scaler.fit_transform(x_train[x_train.columns[1:]])\n",
    "#y_scaled = scaler.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaled = x_train[x_train.columns[1:]].to_numpy()\n",
    "y_scaled = y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_scaled[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is better to run this step first\n",
    "y_pred = x_valid[['time_step']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg1 = LinearRegression().fit(x_scaled, y_scaled[:,0])\n",
    "y_pred['washing_machine'] = reg1.predict(x_valid[X_vars_set])\n",
    "\n",
    "reg2 = LinearRegression().fit(x_scaled, y_scaled[:,1])\n",
    "y_pred['fridge_freezer'] = reg2.predict(x_valid[X_vars_set])\n",
    "\n",
    "reg3 = LinearRegression().fit(x_scaled, y_scaled[:,2])\n",
    "y_pred['TV'] = reg3.predict(x_valid[X_vars_set])\n",
    "\n",
    "reg4 = LinearRegression().fit(x_scaled, y_scaled[:,3])\n",
    "y_pred['kettle'] = reg4.predict(x_valid[X_vars_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_nilm(y_valid, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. ELM / Hierarchical-ELMs (H-ELMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Parameters\n",
    "    ----------\n",
    "    `n_hidden` : int, optional (default=20)\n",
    "        Number of units to generate in the SimpleRandomLayer\n",
    "    `alpha` : float, optional (default=0.5)\n",
    "        Mixing coefficient for distance and dot product input activations:\n",
    "        activation = alpha*mlp_activation + (1-alpha)*rbf_width*rbf_activation\n",
    "    `rbf_width` : float, optional (default=1.0)\n",
    "        multiplier on rbf_activation\n",
    "    `activation_func` : {callable, string} optional (default='tanh')\n",
    "        Function used to transform input activation\n",
    "        It must be one of 'tanh', 'sine', 'tribas', 'inv_tribase', 'sigmoid',\n",
    "        'hardlim', 'softlim', 'gaussian', 'multiquadric', 'inv_multiquadric' or\n",
    "        a callable.  If none is given, 'tanh' will be used. If a callable\n",
    "        is given, it will be used to compute the hidden unit activations.\n",
    "    `activation_args` : dictionary, optional (default=None)\n",
    "        Supplies keyword arguments for a callable activation_func\n",
    "    `user_components`: dictionary, optional (default=None)\n",
    "        dictionary containing values for components that woud otherwise be\n",
    "        randomly generated.  Valid key/value pairs are as follows:\n",
    "           'radii'  : array-like of shape [n_hidden]\n",
    "           'centers': array-like of shape [n_hidden, n_features]\n",
    "           'biases' : array-like of shape [n_hidden]\n",
    "           'weights': array-like of shape [n_hidden, n_features]\n",
    "    `regressor`    : regressor instance, optional (default=None)\n",
    "        If provided, this object is used to perform the regression from hidden\n",
    "        unit activations to the outputs and subsequent predictions.  If not\n",
    "        present, an ordinary linear least squares fit is performed\n",
    "    `random_state`  : int, RandomState instance or None (default=None)\n",
    "        Control the pseudo random number generator used to generate the\n",
    "        hidden unit weights at fit time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_vars_set = X_vars_base+X_vars_3mins+X_vars_day+X_vars_hour2+X_vars_weather2+X_vars_window3\n",
    "X_vars_set = X_vars_base+X_vars_3mins+X_vars_day+X_vars_hour2+X_vars_minute+X_vars_weather2+X_vars_window3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_notnull[['time_step']+X_vars_set]\n",
    "y = train_notnull[y_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1)\n",
    "print('Train size: {train}, Validation size: {test}'.format(train=x_train.shape[0], test=x_valid.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler = StandardScaler()\n",
    "#x_scaled = scaler.fit_transform(x_train[x_train.columns[1:]])\n",
    "#y_scaled = scaler.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaled = x_train[x_train.columns[1:]].to_numpy()\n",
    "y_scaled = y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is better to run this step first\n",
    "y_pred = x_valid[['time_step']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmr = ELMRegressor(random_state=24, n_hidden=128, activation_func='multiquadric', alpha=0.7)\n",
    "\n",
    "elmr1 = elmr.fit(x_scaled, y_scaled[:,0])\n",
    "y_pred['washing_machine'] = elmr1.predict(x_valid[X_vars_set])\n",
    "\n",
    "elmr2 = elmr.fit(x_scaled, y_scaled[:,1])\n",
    "y_pred['fridge_freezer'] = elmr2.predict(x_valid[X_vars_set])\n",
    "\n",
    "elmr3 = elmr.fit(x_scaled, y_scaled[:,2])\n",
    "y_pred['TV'] = elmr3.predict(x_valid[X_vars_set])\n",
    "\n",
    "elmr4 = elmr.fit(x_scaled, y_scaled[:,3])\n",
    "y_pred['kettle'] = elmr4.predict(x_valid[X_vars_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_nilm(y_valid, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmr = pipeline.Pipeline([('rhl', RandomLayer(random_state=0, n_hidden=256, activation_func='multiquadric', alpha=0.7)),\n",
    "                          ('rh2', RandomLayer(random_state=42, n_hidden=256, activation_func='multiquadric', alpha=0.5)),\n",
    "                          ('lr', LinearRegression(fit_intercept=True))])\n",
    "\n",
    "elmr1 = elmr.fit(x_scaled, y_scaled[:,0])\n",
    "y_pred['washing_machine'] = elmr1.predict(x_valid[X_vars_set])\n",
    "\n",
    "elmr2 = elmr.fit(x_scaled, y_scaled[:,1])\n",
    "y_pred['fridge_freezer'] = elmr2.predict(x_valid[X_vars_set])\n",
    "\n",
    "elmr3 = elmr.fit(x_scaled, y_scaled[:,2])\n",
    "y_pred['TV'] = elmr3.predict(x_valid[X_vars_set])\n",
    "\n",
    "elmr4 = elmr.fit(x_scaled, y_scaled[:,3])\n",
    "y_pred['kettle'] = elmr4.predict(x_valid[X_vars_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_nilm(y_valid, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters\n",
    "  - n_estimators (int) – Number of gradient boosted trees. Equivalent to number of boosting rounds.\n",
    "  - max_depth (int) – Maximum tree depth for base learners.\n",
    "  - learning_rate (float) – Boosting learning rate (xgb’s “eta”)\n",
    "  - verbosity (int) – The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
    "  - booster (string) – Specify which booster to use: gbtree, gblinear or dart.\n",
    "  - tree_method (string) – Specify which tree method to use. Default to auto. If this parameter is set to default, XGBoost will choose the most conservative option available. It’s recommended to study this option from parameters document.\n",
    "  - n_jobs (int) – Number of parallel threads used to run xgboost.\n",
    "  - gamma (float) – Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
    "  - min_child_weight (int) – Minimum sum of instance weight(hessian) needed in a child.\n",
    "  - max_delta_step (int) – Maximum delta step we allow each tree’s weight estimation to be.\n",
    "  - subsample (float) – Subsample ratio of the training instance.\n",
    "  - colsample_bytree (float) – Subsample ratio of columns when constructing each tree.\n",
    "  - colsample_bylevel (float) – Subsample ratio of columns for each level.\n",
    "  - colsample_bynode (float) – Subsample ratio of columns for each split.\n",
    "  - reg_alpha (float (xgb's alpha)) – L1 regularization term on weights\n",
    "  - reg_lambda (float (xgb's lambda)) – L2 regularization term on weights\n",
    "  - scale_pos_weight (float) – Balancing of positive and negative weights.\n",
    "  - base_score – The initial prediction score of all instances, global bias.\n",
    "  - random_state (int) – Random number seed.\n",
    "  - num_parallel_tree (int) – Used for boosting random forest.\n",
    "  - importance_type (string, default \"gain\") – The feature importance type for the feature_importances_ property: either “gain”, “weight”, “cover”, “total_gain” or “total_cover”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGB_Regressor(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.scr = preprocessing.StandardScaler()\n",
    "        #0.45\n",
    "        self.param ={'objective':'reg:squarederror',\n",
    "                    'max_depth':7,\n",
    "                    'n_estimators':1000,\n",
    "                    'n_jobs':4,\n",
    "                     'min_child_weight': 7\n",
    "                    }\n",
    "        self.reg = xgb.XGBRegressor(**self.param)\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        #X = self.scr.fit_transform(X)\n",
    "        self.reg.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scr.transform(X)\n",
    "        return self.reg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cv(model, X, y, cv_folds=5, early_stopping_rounds=50, seed=42):\n",
    "    xgb_param = model.get_xgb_params()\n",
    "    xgtrain = xgb.DMatrix(X, label=y)\n",
    "    cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=model.get_params()['n_estimators'], nfold=cv_folds,\n",
    "                    metrics='rmse', seed=seed, callbacks=[\n",
    "            xgb.callback.print_evaluation(show_stdv=False),\n",
    "            xgb.callback.early_stop(early_stopping_rounds)\n",
    "       ])\n",
    "    num_round_best = cvresult.shape[0] - 1\n",
    "    print('Best round num: ', num_round_best)\n",
    "    return num_round_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vars_set = X_vars_base+X_vars_3mins+X_vars_day+X_vars_hour1+X_vars_hour2+X_vars_minute+X_vars_weather1+X_vars_weather2+X_vars_window1+X_vars_window2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_notnull[['time_step']+X_vars_set]\n",
    "y = train_notnull[y_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1)\n",
    "print('Train size: {train}, Validation size: {test}'.format(train=x_train.shape[0], test=x_valid.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scr = preprocessing.StandardScaler()\n",
    "#x_scaled = scr.fit_transform(x_train[x_train.columns[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = x_train[x_train.columns[1:]].to_numpy()\n",
    "y_scaled = y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'objective':'reg:squarederror',\n",
    "                    'max_depth':5,\n",
    "                    'n_estimators':500,\n",
    "                    'n_jobs':4,\n",
    "                    'min_child_weight':8\n",
    "                    }\n",
    "clf = xgb.XGBRegressor(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = clf.fit(x_scaled, y_scaled[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = clf.fit(x_scaled, y_train[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3 = clf.fit(x_scaled, y_train[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4 = clf.fit(x_scaled, y_train[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = x_valid[['time_step']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid_scaled = x_valid[x_valid.columns[1:]].to_numpy()\n",
    "#x_valid_scaled = scr.fit_transform(x_valid[x_valid.columns[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred['washing_machine'] = clf1.predict(x_valid_scaled)\n",
    "y_pred['fridge_freezer'] = clf2.predict(x_valid_scaled)\n",
    "y_pred['TV'] = clf3.predict(x_valid_scaled)\n",
    "y_pred['kettle'] = clf4.predict(x_valid_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_nilm(y_valid, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
